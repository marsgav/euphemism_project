{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "273735e3",
   "metadata": {},
   "source": [
    "The code in this notebook was for the following:\n",
    "- cropping dataset texts to the sentence containing the PET (to better focus the sentiment analysis)\n",
    "- creating a separate file where PETs are replaced with their literal meaning, based off the list of PETs used\n",
    "- using roBERTa to compute and compare sentiment \n",
    "- grouping the sentiment scores by keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edfd53e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from utils import *\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2e696c",
   "metadata": {},
   "source": [
    "## Cropping the texts to a single sentence before analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218ee286",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"total_1s_max30.csv\", encoding = \"utf-8\")\n",
    "\n",
    "print('There are',len(df), 'total lines in the data')\n",
    "\n",
    "df = df[['keyword','orig_text']]\n",
    "df = pd.DataFrame(df)\n",
    "df.columns = ['keyword','text']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64cbabba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove HTML tags and @ signs\n",
    "df['text'] = df['text'].str.replace('( <.*?>|&lt;.*?&gt;)', '')\n",
    "df['text'] = df['text'].str.replace('@', '')\n",
    "\n",
    "# Replace corpus question marks occurring between 2 lowercase letters with an apostrophe\n",
    "df['text'] = df['text'].str.replace('(?<=([a-z]|I)) \\? (?=[a-z])', ' \\'')\n",
    "\n",
    "# Replace isolated periods, question marks, exclamation marks and periods + quotation marks with a sentence boundary <s>\n",
    "df['text'] = df['text'].str.replace(' \\.( |$)(?=\\\"?([A-Z]|$))', ' . <s> ')\n",
    "df['text'] = df['text'].str.replace(' \\?( |$)(?=\\\"?([A-Z]|$))', ' ? <s> ')\n",
    "df['text'] = df['text'].str.replace(' \\!( |$)(?=\\\"?([A-Z]|$))', ' ! <s> ')\n",
    "df['text'] = df['text'].str.replace(' \\!( |$)(?=\\\"?([A-Z]|$))', ' ! <s> ')\n",
    "\n",
    "# Treat hyphens and slashes as separate tokens (e.g. to identify \"chest-thumping\" or \"overweight/obese\")\n",
    "df['text'] = df['text'].str.replace('-', ' - ')\n",
    "df['text'] = df['text'].str.replace('/', ' / ')\n",
    "\n",
    "pd.set_option('display.max_colwidth', 0) # Wrap text when viewing df\n",
    "\n",
    "# df # shows the preprocessed / sentence-separated text\n",
    "\n",
    "# Here we do the actual cropping, going through each row in the df:\n",
    "for i, row in df.iterrows():\n",
    "    text = df.loc[i, 'text']\n",
    "    keyword = df.loc[i, 'keyword']\n",
    "    df.loc[i, 'text'] = get_single_sentence_context(text, keyword)\n",
    "\n",
    "# df # shows the cropped and tagged text\n",
    "\n",
    "# The code below removes the sentence boundary tags that were put in, undoes the preprocessing tasks, and\n",
    "# attempts to clean up spacing (however, the spacing can remain messy in some of the cases)\n",
    "\n",
    "# remove <s> tags\n",
    "df['text'] = df['text'].str.replace(r' <s>', r'')\n",
    "\n",
    "# remove opening/closing spaces between parens/quotes, and before punctuation marks\n",
    "df['text'] = df['text'].str.replace(r'\\( (.*?) \\)', r'(\\1)')\n",
    "df['text'] = df['text'].str.replace(r'\"\\s(.*?)\\s\"', r'\"\\1\"')\n",
    "df['text'] = df['text'].str.replace(r'\\s([.,?!:;\\'])', r'\\1')\n",
    "\n",
    "# remove spaces before contractions\n",
    "df['text'] = df['text'].str.replace(r' (?!I)([A-Za-z]\\'[A-Za-z]+)', r'\\1')\n",
    "\n",
    "# undo spaces around hyphens and slashes\n",
    "df['text'] = df['text'].str.replace(r'\\s-\\s', r'-')\n",
    "df['text'] = df['text'].str.replace(r'\\s/\\s', r'/')\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48228672",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('total_1s_max30_cropped.csv') # At this point, may need to manually crop a few texts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9c388b",
   "metadata": {},
   "source": [
    "## Substituting in literal meanings for sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdfae972",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('total_1s_max30_cropped.csv', index_col = 0, encoding = \"utf-8\")\n",
    "euph_list = pd.read_csv(\"combined_euphs_1-6.csv\", encoding = \"utf-8\")\n",
    "\n",
    "df['literal'] = \"\" # supply each row with the literal meaning of the keyword, based off euph_list\n",
    "for i, row in df.iterrows():\n",
    "    text = df.loc[i, 'text']\n",
    "    keyword = df.loc[i, 'keyword']\n",
    "    # locate the keywords with multiple literal meanings; supply literal meaning manually for these\n",
    "    if (keyword == \"accident\" or keyword == \"put to sleep\" or keyword == \"seeing someone\"):\n",
    "        # print(keyword + \" located at index \" + str(i))\n",
    "        continue\n",
    "    else:\n",
    "        literal = euph_list.loc[euph_list.euphemism == keyword]\n",
    "        literal_interp = literal['real_meaning'].item()\n",
    "        df.loc[i, 'literal'] = literal_interp\n",
    "df\n",
    "df.to_csv('with_keywords_temp.csv') # in this file, supply the literal meanings of certain phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963a5218",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"with_keywords_temp.csv\", index_col = 0, encoding = \"utf-8\") # AFTER labelling the literal meanings\n",
    "for i, row in df.iterrows():\n",
    "    text = df.loc[i, 'text']\n",
    "    keyword = df.loc[i, 'keyword']\n",
    "    literal = df.loc[i, 'literal']\n",
    "    orig = df.loc[i, 'text']\n",
    "    pattern = re.compile(keyword, re.IGNORECASE)\n",
    "    df.loc[i, 'text'] = pattern.sub(literal, text)\n",
    "    # df.loc[i, 'text'] = text.replace(keyword, literal) # old; couldn't do case-insensitive\n",
    "    if (df.loc[i, 'text'] == orig):\n",
    "        print(i)\n",
    "df\n",
    "df.to_csv('with_substitutions_temp.csv') # The examples at the indices listed need their literal meanings manually subbed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0868dc80",
   "metadata": {},
   "source": [
    "## Loading the roBERTa model for sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1877d25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "euphs = pd.read_csv(\"euph_texts_1-6.csv\", index_col = 0, encoding = \"utf-8\")\n",
    "literals = pd.read_csv(\"literal_texts_1-6.csv\", index_col = 0, encoding = \"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80df12d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "import csv\n",
    "import urllib.request\n",
    "\n",
    "def load_roberta_sentiment():\n",
    "    # Tasks:\n",
    "    # emoji, emotion, hate, irony, offensive, sentiment\n",
    "    # stance/abortion, stance/atheism, stance/climate, stance/feminist, stance/hillary\n",
    "\n",
    "    task='sentiment'\n",
    "    MODEL = f\"cardiffnlp/twitter-roberta-base-{task}\"\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "\n",
    "    # download label mapping\n",
    "    labels=[]\n",
    "    mapping_link = f\"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/{task}/mapping.txt\"\n",
    "    with urllib.request.urlopen(mapping_link) as f:\n",
    "        html = f.read().decode('utf-8').split(\"\\n\")\n",
    "        csvreader = csv.reader(html, delimiter='\\t')\n",
    "    labels = [row[1] for row in csvreader if len(row) > 1]\n",
    "\n",
    "    # PT\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(MODEL)\n",
    "    model.save_pretrained(MODEL)\n",
    "    tokenizer.save_pretrained(MODEL)\n",
    "    \n",
    "    return labels, model, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8fece6",
   "metadata": {},
   "source": [
    "### Using the sentiment analysis on the euphs and literals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b9b28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "euphs['neutral'] = 0\n",
    "euphs['positive'] = 0\n",
    "euphs['negative'] = 0\n",
    "\n",
    "labels, model, tokenizer = load_roberta_sentiment()\n",
    "\n",
    "for i, row in euphs.iterrows():\n",
    "    text = euphs.loc[i, 'text']\n",
    "    keyword = euphs.loc[i, 'keyword']\n",
    "    encoded_input = tokenizer(text, return_tensors='pt')\n",
    "    \n",
    "    output = model(**encoded_input)\n",
    "    scores = output[0][0].detach().numpy()\n",
    "    scores = softmax(scores)\n",
    "    ranking = np.argsort(scores)\n",
    "    ranking = ranking[::-1]\n",
    "    #print(labels[ranking[0]])\n",
    "    \n",
    "    euphs.loc[i, labels[ranking[0]]] = scores[ranking[0]]\n",
    "    euphs.loc[i, labels[ranking[1]]] = scores[ranking[1]]\n",
    "    #print(labels[ranking[2]])\n",
    "    euphs.loc[i, labels[ranking[2]]] = scores[ranking[2]]\n",
    "\n",
    "euphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860bb1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "literals['neutral'] = 0\n",
    "literals['positive'] = 0\n",
    "literals['negative'] = 0\n",
    "for i, row in literals.iterrows():\n",
    "    text = literals.loc[i, 'text']\n",
    "    keyword = literals.loc[i, 'keyword']\n",
    "    encoded_input = tokenizer(text, return_tensors='pt')\n",
    "    \n",
    "    output = model(**encoded_input)\n",
    "    scores = output[0][0].detach().numpy()\n",
    "    scores = softmax(scores)\n",
    "    ranking = np.argsort(scores)\n",
    "    ranking = ranking[::-1]\n",
    "    \n",
    "    literals.loc[i, labels[ranking[0]]] = scores[ranking[0]]\n",
    "    literals.loc[i, labels[ranking[1]]] = scores[ranking[1]]\n",
    "    literals.loc[i, labels[ranking[2]]] = scores[ranking[2]]\n",
    "\n",
    "literals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6a8031",
   "metadata": {},
   "outputs": [],
   "source": [
    "euphs.to_csv('euphs_roBERTa_checkpoint.csv')\n",
    "literals.to_csv('literals_roBERTa_checkpoint.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431976bc",
   "metadata": {},
   "source": [
    "## Loading roBERTa for offensiveness analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa28208",
   "metadata": {},
   "outputs": [],
   "source": [
    "euphs = pd.read_csv(\"euphs_roBERTa_checkpoint.csv\", index_col = 0, encoding = \"utf-8\").reset_index(drop=True)\n",
    "literals = pd.read_csv(\"literals_roBERTa_checkpoint.csv\", index_col = 0, encoding = \"utf-8\").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b07dc09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_roberta_offensive():\n",
    "    task='offensive'\n",
    "    MODEL = f\"cardiffnlp/twitter-roberta-base-{task}\"\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "\n",
    "    # download label mapping\n",
    "    labels=[]\n",
    "    mapping_link = f\"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/{task}/mapping.txt\"\n",
    "    with urllib.request.urlopen(mapping_link) as f:\n",
    "        html = f.read().decode('utf-8').split(\"\\n\")\n",
    "        csvreader = csv.reader(html, delimiter='\\t')\n",
    "    labels = [row[1] for row in csvreader if len(row) > 1]\n",
    "\n",
    "    # PT\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(MODEL)\n",
    "    model.save_pretrained(MODEL)\n",
    "    tokenizer.save_pretrained(MODEL)\n",
    "    \n",
    "    return labels, model, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ed2370",
   "metadata": {},
   "source": [
    "### Using the offensiveness analysis on the euphs and literals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9f7d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "euphs['offensive'] = 0\n",
    "euphs['not-offensive'] = 0\n",
    "\n",
    "labels, model, tokenizer = load_roberta_offensive()\n",
    "\n",
    "for i, row in euphs.iterrows():\n",
    "    text = euphs.loc[i, 'text']\n",
    "    keyword = euphs.loc[i, 'keyword']\n",
    "    encoded_input = tokenizer(text, return_tensors='pt')\n",
    "    \n",
    "    output = model(**encoded_input)\n",
    "    scores = output[0][0].detach().numpy()\n",
    "    scores = softmax(scores)\n",
    "    ranking = np.argsort(scores)\n",
    "    ranking = ranking[::-1]\n",
    "    #print(labels[ranking[0]])\n",
    "    \n",
    "    euphs.loc[i, labels[ranking[0]]] = scores[ranking[0]]\n",
    "    euphs.loc[i, labels[ranking[1]]] = scores[ranking[1]]\n",
    "    #print(labels[ranking[2]])\n",
    "    # euphs.loc[i, labels[ranking[2]]] = scores[ranking[2]]\n",
    "\n",
    "euphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e9e6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "literals['offensive'] = 0\n",
    "literals['not-offensive'] = 0\n",
    "for i, row in literals.iterrows():\n",
    "    text = literals.loc[i, 'text']\n",
    "    keyword = literals.loc[i, 'keyword']\n",
    "    encoded_input = tokenizer(text, return_tensors='pt')\n",
    "    \n",
    "    output = model(**encoded_input)\n",
    "    scores = output[0][0].detach().numpy()\n",
    "    scores = softmax(scores)\n",
    "    ranking = np.argsort(scores)\n",
    "    ranking = ranking[::-1]\n",
    "    #print(labels[ranking[0]])\n",
    "    \n",
    "    literals.loc[i, labels[ranking[0]]] = scores[ranking[0]]\n",
    "    literals.loc[i, labels[ranking[1]]] = scores[ranking[1]]\n",
    "    #print(labels[ranking[2]])\n",
    "    # euphs.loc[i, labels[ranking[2]]] = scores[ranking[2]]\n",
    "\n",
    "literals\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21563b5",
   "metadata": {},
   "source": [
    "### For computing differences in sentiment/offensiveness values when substituting literal meanings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8390da",
   "metadata": {},
   "outputs": [],
   "source": [
    "differences = euphs[['keyword','text', 'literal']]\n",
    "differences['neu_diff'] = 0\n",
    "differences['pos_diff'] = 0\n",
    "differences['neg_diff'] = 0\n",
    "differences['off_diff'] = 0\n",
    "differences['n-off_diff'] = 0\n",
    "differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0144f63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in differences.iterrows():\n",
    "    differences.loc[i, 'neu_diff'] = (literals.loc[i, 'neutral'] - euphs.loc[i, 'neutral'])/euphs.loc[i, 'neutral']\n",
    "    differences.loc[i, 'pos_diff'] = (literals.loc[i, 'positive'] - euphs.loc[i, 'positive'])/euphs.loc[i, 'positive']\n",
    "    differences.loc[i, 'neg_diff'] = (literals.loc[i, 'negative'] - euphs.loc[i, 'negative'])/euphs.loc[i, 'negative']\n",
    "    differences.loc[i, 'off_diff'] = (literals.loc[i, 'offensive'] - euphs.loc[i, 'offensive'])/euphs.loc[i, 'offensive']\n",
    "    differences.loc[i, 'n-off_diff'] = (literals.loc[i, 'not-offensive'] - euphs.loc[i, 'not-offensive'])/euphs.loc[i, 'not-offensive']\n",
    "    \n",
    "differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708b3784",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Mean % change in neutral scores: ', differences['neu_diff'].mean())\n",
    "print('Mean % change in positive scores: ', differences['pos_diff'].mean())\n",
    "print('Mean % change in negative scores: ', differences['neg_diff'].mean())\n",
    "print('Mean % change in offensive scores: ', differences['off_diff'].mean())\n",
    "print('Mean % change in not-offensive scores: ', differences['n-off_diff'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afaa33c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "differences.to_csv('sentiment_diffs.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f7bae1",
   "metadata": {},
   "source": [
    "### Looking at sentiment/offensiveness differences by keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46fd85f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_keywords = pd.read_csv('sentiment_diffs.csv', index_col = 0, encoding = 'utf-8')\n",
    "diff_keywords = diff_keywords.groupby('type')['neu_diff', 'pos_diff', 'neg_diff', 'off_diff', 'n-off_diff'].mean()\n",
    "display(diff_keywords)\n",
    "diff_keywords.to_csv('sentiment_diffs_by_type.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0e90fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
